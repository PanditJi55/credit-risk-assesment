# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import r2_score, accuracy_score, classification_report, precision_recall_fscore_support
from scipy.stats import chi2_contingency, f_oneway
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
import xgboost as xgb
import warnings
import os

warnings.filterwarnings("ignore")

# Load the dataset
a1 = pd.read_excel("case_study1.xlsx")
a2 = pd.read_excel("case_study2.xlsx")

df1 = a1.copy()
df2 = a2.copy()

# Remove nulls
df1 = df1.loc[df1['Age_Oldest_TL'] != -99999]

columns_to_be_removed = []
for i in df2.columns:
    if df2.loc[df2[i] == -99999].shape[0] > 10000:
        columns_to_be_removed.append(i)

df2 = df2.drop(columns_to_be_removed, axis=1)

for i in df2.columns:
    df2 = df2.loc[df2[i] != -99999]

# Checking common column names
common_columns = [i for i in df1.columns if i in df2.columns]
print("Common columns:", common_columns)

# Merge the two dataframes, inner join so that no nulls are present
df = pd.merge(df1, df2, how='inner', on='PROSPECTID')

# Check how many columns are categorical
categorical_columns = [i for i in df.columns if df[i].dtype == 'object']
print("Categorical columns:", categorical_columns)

# Chi-square test
for i in ['MARITALSTATUS', 'EDUCATION', 'GENDER', 'last_prod_enq2', 'first_prod_enq2']:
    chi2, pval, _, _ = chi2_contingency(pd.crosstab(df[i], df['Approved_Flag']))
    print(i, '---', pval)

# VIF for numerical columns
numeric_columns = [i for i in df.columns if df[i].dtype != 'object' and i not in ['PROSPECTID', 'Approved_Flag']]
vif_data = df[numeric_columns]
columns_to_be_kept = []

for i in range(vif_data.shape[1]):
    vif_value = variance_inflation_factor(vif_data.values, i)
    print(f"{numeric_columns[i]} --- VIF: {vif_value}")
    if vif_value <= 6:
        columns_to_be_kept.append(numeric_columns[i])

# ANOVA for numerical columns
columns_to_be_kept_numerical = []

for i in columns_to_be_kept:
    group_values = [df.loc[df['Approved_Flag'] == f'P{j + 1}', i].values for j in range(4)]
    f_statistic, p_value = f_oneway(*group_values)

    if p_value <= 0.05:
        columns_to_be_kept_numerical.append(i)

# Feature selection is done for categorical and numerical features
features = columns_to_be_kept_numerical + ['MARITALSTATUS', 'EDUCATION', 'GENDER', 'last_prod_enq2', 'first_prod_enq2']
df = df[features + ['Approved_Flag']]

# Label encoding for the categorical features
education_mapping = {
    'SSC': 1, '12TH': 2, 'GRADUATE': 3, 'UNDER GRADUATE': 3,
    'POST-GRADUATE': 4, 'OTHERS': 1, 'PROFESSIONAL': 3
}
df['EDUCATION'] = df['EDUCATION'].map(education_mapping).astype(int)

df_encoded = pd.get_dummies(df, columns=['MARITALSTATUS', 'GENDER', 'last_prod_enq2', 'first_prod_enq2'])

# Machine Learning Model Fitting
y = df_encoded['Approved_Flag']
x = df_encoded.drop(['Approved_Flag'], axis=1)

# Random Forest Classifier
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
rf_classifier = RandomForestClassifier(n_estimators=200, random_state=42)
rf_classifier.fit(x_train, y_train)
y_pred = rf_classifier.predict(x_test)

accuracy = accuracy_score(y_test, y_pred)
print(f'Random Forest Accuracy: {accuracy:.2f}')
precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred)

for i, v in enumerate(['p1', 'p2', 'p3', 'p4']):
    print(f"Class {v} - Precision: {precision[i]}, Recall: {recall[i]}, F1 Score: {f1_score[i]}")

# XGBoost Classifier
xgb_classifier = xgb.XGBClassifier(objective='multi:softmax', num_class=4)
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

x_train, x_test, y_train, y_test = train_test_split(x, y_encoded, test_size=0.2, random_state=42)
xgb_classifier.fit(x_train, y_train)
y_pred = xgb_classifier.predict(x_test)

accuracy = accuracy_score(y_test, y_pred)
print(f'XGBoost Accuracy: {accuracy:.2f}')
precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred)

for i, v in enumerate(['p1', 'p2', 'p3', 'p4']):
    print(f"Class {v} - Precision: {precision[i]}, Recall: {recall[i]}, F1 Score: {f1_score[i]}")

# Decision Tree Classifier
dt_model = DecisionTreeClassifier(max_depth=20, min_samples_split=10)
dt_model.fit(x_train, y_train)
y_pred = dt_model.predict(x_test)

accuracy = accuracy_score(y_test, y_pred)
print(f"Decision Tree Accuracy: {accuracy:.2f}")
precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred)

for i, v in enumerate(['p1', 'p2', 'p3', 'p4']):
    print(f"Class {v} - Precision: {precision[i]}, Recall: {recall[i]}, F1 Score: {f1_score[i]}")

# Apply standard scaler
columns_to_be_scaled = ['Age_Oldest_TL', 'Age_Newest_TL', 'time_since_recent_payment',
                        'max_recent_level_of_deliq', 'recent_level_of_deliq',
                        'time_since_recent_enq', 'NETMONTHLYINCOME', 'Time_With_Curr_Empr']

scaler = StandardScaler()
df_encoded[columns_to_be_scaled] = scaler.fit_transform(df_encoded[columns_to_be_scaled])

# Hyperparameter tuning for XGBoost
param_grid = {
    'colsample_bytree': [0.1, 0.3, 0.5, 0.7, 0.9],
    'learning_rate': [0.001, 0.01, 0.1, 1],
    'max_depth': [3, 5, 8, 10],
    'alpha': [1, 10, 100],
    'n_estimators': [10, 50, 100]
}

# Initialize Grid Search results
answers_grid = {
    'combination': [], 'train_Accuracy': [], 'test_Accuracy': [],
    'colsample_bytree': [], 'learning_rate': [], 'max_depth': [],
    'alpha': [], 'n_estimators': []
}

index = 0

# Loop through each combination of hyperparameters
for colsample_bytree in param_grid['colsample_bytree']:
    for learning_rate in param_grid['learning_rate']:
        for max_depth in param_grid['max_depth']:
            for alpha in param_grid['alpha']:
                for n_estimators in param_grid['n_estimators']:
                    index += 1
                    model = xgb.XGBClassifier(
                        objective='multi:softmax', num_class=4,
                        colsample_bytree=colsample_bytree, learning_rate=learning_rate,
                        max_depth=max_depth, alpha=alpha, n_estimators=n_estimators
                    )
                    model.fit(x_train, y_train)

                    # Predict on training and testing sets
                    y_pred_train = model.predict(x_train)
                    y_pred_test = model.predict(x_test)

                    # Calculate train and test results
                    train_accuracy = accuracy_score(y_train, y_pred_train)
                    test_accuracy = accuracy_score(y_test, y_pred_test)

                    # Include results in the list
                    answers_grid['combination'].append(index)
                    answers_grid['train_Accuracy'].append(train_accuracy)
                    answers_grid['test_Accuracy'].append(test_accuracy)
                    answers_grid['colsample_bytree'].append(colsample_bytree)
                    answers_grid['learning_rate'].append(learning_rate)
                    answers_grid['max_depth'].append(max_depth)
                    answers_grid['alpha'].append(alpha)
                    answers_grid['n_estimators'].append(n_estimators)

                    # Print results for this combination
                    print(f"Combination {index}")
                    print(f"colsample_bytree: {colsample_bytree}, learning_rate: {learning_rate}")
                    print(f"max_depth: {max_depth}, alpha: {alpha}, n_estimators: {n_estimators}")
                    print(f"Training Accuracy: {train_accuracy}, Testing Accuracy: {test_accuracy}")
                    print('------------------------------------')

# Convert the results dictionary to a DataFrame
results_df = pd.DataFrame(answers_grid)
